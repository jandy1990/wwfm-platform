# Claude Ã— Gemini Hybrid Quality System

## ğŸ¯ Overview

This hybrid system leverages Gemini for bulk generation (free/cheap) and Claude for quality checking (targeted spend), achieving 95%+ quality at 5% of the cost of pure Claude generation.

**Cost Comparison:**
- Pure Claude: $137 for 32,000 solutions
- Hybrid System: ~$30-40 total ($0-30 Gemini + $5-10 Claude)
- Savings: **70-75% cost reduction**

## ğŸš€ Quick Start

### 1. Apply Database Migration

```bash
npm run quality:migrate
```

Or manually run the SQL in Supabase:
```sql
-- See database/migrations/add-quality-tracking.sql
```

### 2. Generate Solutions with Gemini

```bash
# Generate solutions with Attribution Required strategy
npm run generate:ai-solutions

# Or with smart selection for better coverage
npm run generate:smart
```

### 3. Run Quality Checks

```bash
# Test with a small batch first
npm run quality:test

# Run full quality checking pipeline
npm run quality:check

# Or start the automated orchestrator
npm run quality:orchestrate
```

### 4. Monitor Progress

```bash
# View real-time dashboard
npm run quality:dashboard

# Auto-refresh mode
npm run quality:dashboard -- --watch
```

## ğŸ“Š Quality Dimensions

The system evaluates solutions across 5 dimensions:

1. **Conversational Completeness (30%)**: Can a friend act on this recommendation?
2. **Evidence Alignment (25%)**: Is the effectiveness rating justified?
3. **Accessibility Truth (20%)**: Are costs and barriers fully disclosed?
4. **Expectation Accuracy (15%)**: Are time/effort expectations realistic?
5. **Category Accuracy (10%)**: Is it in the right category?

**Pass Threshold**: All dimensions must score â‰¥85%

## ğŸ”§ Commands

### `npm run quality:test`
Test with a small batch (10 solutions) without applying fixes

Options:
- `-s, --size <number>`: Batch size (default: 10)

### `npm run quality:check`
Run quality checks and apply fixes

Options:
- `-b, --batch-size <number>`: Solutions per batch (default: 100)
- `-l, --limit <number>`: Maximum batches (default: 10)
- `-c, --cost-limit <number>`: Maximum cost in USD (default: 10.0)

### `npm run quality:orchestrate`
Start automated quality checking with triggers

Options:
- `-b, --batch-size <number>`: Solutions per batch (default: 100)
- `-t, --trigger <number>`: Trigger after N pending (default: 100)
- `-i, --interval <hours>`: Check interval (default: 6)
- `-q, --quality <percent>`: Quality threshold (default: 80)
- `-c, --cost-limit <number>`: Daily cost limit (default: 10.0)
- `--no-auto`: Disable automatic triggers

### `npm run quality:dashboard`
Display quality metrics dashboard

Options:
- `-w, --watch`: Auto-refresh every 30 seconds

## ğŸ—ï¸ Architecture

```
Gemini Generation â†’ Database â†’ Quality Checker â†’ Claude API â†’ Fixes Applied
                       â†‘                              â†“
                   Orchestrator â† â† â† â† â† â† â† Cost Tracker
                       â†“
                   Dashboard
```

### Components

1. **ClaudeQualityChecker** (`services/claude-quality-checker.ts`)
   - Evaluates solutions in batches
   - Returns specific fixes
   - Applies updates to database

2. **QualityOrchestrator** (`services/quality-orchestrator.ts`)
   - Monitors pending solutions
   - Triggers checks based on rules
   - Tracks costs and metrics

3. **Quality Pipeline CLI** (`quality-pipeline.ts`)
   - Command-line interface
   - Dashboard visualization
   - Testing capabilities

## ğŸ“ˆ Metrics & Monitoring

The dashboard displays:
- Quality scores for each dimension
- Solution counts (pending/passed/fixed/failed)
- Cost metrics (today/week/month)
- Processing speed and time remaining
- System status and configuration

## ğŸ’° Cost Management

The system includes built-in cost controls:
- Daily spending limits
- Cost tracking per batch
- Projected monthly spend
- Cost per solution metrics

Default limits:
- $10/day maximum spend
- 100 solutions per batch
- Automatic stop at limit

## ğŸ” Quality Issues & Fixes

Common issues the system catches and fixes:

1. **Missing Starting Points**
   - Before: "Try meditation"
   - After: "Download Headspace app from App Store"

2. **Inflated Effectiveness**
   - Before: 5.0 stars with weak evidence
   - After: 4.2 stars with proper justification

3. **Hidden Costs**
   - Before: No cost mentioned
   - After: "$15/month subscription required"

4. **Unrealistic Timeframes**
   - Before: "See results immediately"
   - After: "3-4 weeks for noticeable improvement"

5. **Wrong Categories**
   - Before: "Headspace" in books_courses
   - After: "Headspace" in apps_software

## ğŸ¯ Expected Outcomes

After running the quality pipeline:
- **95%+ solutions pass** all quality dimensions
- **90%+ conversation completeness** (friend can act on it)
- **100% category accuracy**
- **No inflated ratings** without evidence
- **All costs disclosed** upfront

## ğŸš¨ Troubleshooting

### "ANTHROPIC_API_KEY not found"
Add to `.env.local`:
```
ANTHROPIC_API_KEY=<your_anthropic_api_key>
```

### "No pending solutions"
Generate solutions first:
```bash
npm run generate:ai-solutions
```

### "Rate limit exceeded"
The system automatically handles rate limits with delays

### "Cost limit reached"
Increase limit or wait for next day:
```bash
npm run quality:orchestrate -- -c 20.0  # $20 limit
```

## ğŸ“ Database Schema

The system adds these tracking fields:

**solutions table:**
- `generation_source`: 'gemini' or 'claude'
- `quality_status`: pending|checking|passed|fixed|failed
- `conversation_completeness_score`: 0-100
- `evidence_alignment_score`: 0-100
- `accessibility_truth_score`: 0-100
- `expectation_accuracy_score`: 0-100
- `category_accuracy_score`: 0-100
- `quality_issues`: JSON of issues found
- `quality_fixes_applied`: JSON of fixes applied

**quality_check_batches table:**
- Tracks batch processing
- Cost per batch
- Average scores
- Common issues

## ğŸ”„ Workflow

1. **Generate** with Gemini (Attribution Required strategy)
2. **Monitor** pending solutions count
3. **Trigger** quality checks (manual or automatic)
4. **Review** dashboard metrics
5. **Iterate** until quality targets met

## ğŸ“Š Success Metrics

The system is successful when:
- âœ… 95%+ solutions pass all dimensions
- âœ… Average conversation completeness > 90%
- âœ… No inflated effectiveness ratings
- âœ… Total cost < $50
- âœ… Processing completes in < 1 week

## ğŸ‰ Results

With this hybrid system, WWFM gets:
- **Claude-level quality** (95%+)
- **Gemini prices** ($0-30)
- **Automated pipeline** (set and forget)
- **Full transparency** (all metrics tracked)
- **Cost control** (daily limits enforced)
